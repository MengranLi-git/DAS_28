---
title: "**Factors influencing Scots' satisfaction with public transport**"
author:
  - name: '*Group 28: Aishwin Tikku, Mengran Li, Steven Kwok, Shaoquan Li, Shuning Li*'
column_numbers: 3
logoleft_name: "Poster_files/figure-html/UoG_keyline.png"
output: 
  posterdown::posterdown_html:
    self_contained: false
    css: ["css/poster.css"]
bibliography: packages.bib
primary_colour: "#47CCD0"
poster_height: "33in"
knit: pagedown::chrome_print
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(fs)
library(tidymodels)
library(GGally)
library(car)
library(gvlma)
library(skimr)
library(kableExtra)

load("../Data.Rdata")
```

# Introduction

-   We **aim** to find factors related to passengers' satisfaction with public transport to help operator to improve its service.

-   The **data** are from [Scotland's official statistics](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdef%2Fconcept%2Ffolders%2Fthemes%2Ftransport). The theme of Transport contains seven datasets, [Road Transport Expenditure](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Froad-transport-expenditure), [Public Transport](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Fpublic-transport), [Road Vehicles](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Froad-vehicles), [Concessionary Travel Cards](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Fconcessionary-cards), [Road Network and Traffic](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Froad-network-traffic), [Travel to Work and Other Purposes](https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Ftravel-to-work-other). There are 460 observations of 24 variables in this research.

# Methods

-   Summarize table and density plot are illustrated to detect data patterns. The scatter and correlation plots are proposed to explore the relationship among variables. Potential factors are identified through **EDA**.

-   The *Satisfaction* is the response variable, the *DateCode* is the control variable and others are independent variables. A **linear regression** model is applied as Eq.(1):\\begin{equation}y=\\beta_0+\\beta_1x_1+\\dots+\\beta_px_p+\\varepsilon \\label{eq:(1)} \\tag{1} \\end{equation}

-   **Model diagnosis** are carried out to check model assumptions. **Stepwise regression** is applied to select variable with **AIC** as criterion. Compare the selected model with the full model in $adj$ $R^2$, AIC and BIC.

-   The uncertainty of the parameters is determined via **bootstrap** method. The significant variables are verified and their 95% CI are estimated.

# Results

```{r}
fit <- lm(Satisfaction ~ ., data = na.omit(Data[, -1]))
```

## Model diagnosis

[Fig.1]{style="color:#cc0000"} shows the regression diagnosis results of model1 with all variables.

-   [Residuals vs Fitted plot]{style="color:#cc0000"} (left top) shows that there is no systematic correlation between the residual value and the fitting value.

-   [Normal Q-Q plot]{style="color:#cc0000"} (right top) shows the points on the graph fall on a straight line with an angle of 45 degrees, which indicates the assumption of normality is not violated.

-   [Scale-location plot]{style="color:#cc0000"} (left bottom) displays the points around the horizontal line are randomly distributed. The invariant variance assumption is satisfied.

-   [Residuals vs leverage]{style="color:#cc0000"} (right bottom) figures out special observations.

-   According to the correlation plot and VIF, there is obvious **multicollinearity** among variables.

```{r, out.width='80%', fig.cap="\\label{fig:dia} Regression diagnosis results", fig.height = 5, fig.align='center'}
par(mfrow = c(2, 2), mar=c(2,2,2,2))
plot(fit)
```

## Stepwise regression

Stepwise regression technique is applied to model selection. The selected model is as [Table. 1]{style="color:#cc0000"}. 

```{r}
fit2 <- lm(Satisfaction ~ DateCode + Cards + Repair + Work_Bus + 
             School + Health + Work_Train + Train_Stations + Without_Car + 
             Petrol_Diesel, data = na.omit(Data[, -1]))
tidy(fit2) %>% 
  filter(!term %in% c("DateCode2012","DateCode2014","DateCode2015",
                     "DateCode2016","DateCode2017","DateCode2018")) %>%
  kable(caption = 'Model2, selected by Stepwise regression')
```

The variables with high correlation are removed by stepwise regression.

## Model compare

```{r}
glance(fit) %>% 
  full_join(glance(fit2)) %>%
  select(adj.r.squared, AIC, BIC) %>% 
  add_column(model = c("model 1", "model 2"), .before = "adj.r.squared") %>%
  kable(caption = 'Compare selected model by Stepwise regression with the full model')
```

The model2 has higher $adj$ $R^2$ and smaller AIC and BIC compared with the model1 as [Table. 2]{style="color:#cc0000"}. Therefore the **model2** is better.

## Bootstrap

To obtain robust results, bootstrap is developed to the estimation of parameters.

```{r, out.width='80%', fig.cap="Density plots of parameters via bootstrap", fig.height = 5, fig.align='center'}
boot_models <- bootstraps(Data[, -c(1, 3, 4, 8)], times = 1000, apparent = TRUE) %>%
  mutate(
    model = map(splits, ~ lm(Satisfaction ~ DateCode + Cards + Repair + Work_Bus + 
                               School + Health + Work_Train + Train_Stations + Without_Car + 
                               Petrol_Diesel, data = .)),
    coef_info = map(model, tidy)
  )

boot_coefs <- boot_models %>%
  unnest(coef_info)

ci <- int_pctl(boot_models, coef_info)
# significant at 0.05
sig <- ci[sign(ci[, 2]) == sign(ci[, 4]), ] %>% pull(term)

boot_coefs <-  boot_coefs %>%
  mutate(sig = ifelse(term %in% sig, "TRUE", "FALSE")) %>%
  filter(term %in% c("Health", "Petrol_Diesel","Repair", "School", "Train_Stations", "Without_Car", "Work_Bus", "Work_Train"))

boot_coefs2 <- boot_coefs %>%
  group_by(term) %>%
  summarise(est = median(estimate), lower = quantile(estimate, 0.025), upper = quantile(estimate, 0.975))

boot_coefs %>%
  ggplot() +
  geom_density(aes(estimate, fill = sig), alpha = 0.7,  color = "white") +
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = est))+
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = lower), color="Red", lty = 2)+
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = upper), color="Red", lty = 2)+
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = 0), color="Blue", lty = 2)+
  facet_wrap(~term, scales = "free")
```

The density plots of parameters are displayed in [Fig.3]{style="color:#cc0000"}. The variables with orange are not significant while the variables with blue are significant at the $\alpha=0.05$. The blue dashed lines are zero and the orange dashed lines are 95% CI of parameters.

# Conclusion

We can conclude that the **Repair** *(The Percentage Of Roads Needing Repairs)*, **School** *(Child Journeys To School By Walking/Cycling)*, **Train_Stations** *(Number Of Train Stations)*, **Work_Bus** *(Bus Journeys To Work)*, **Work_Train**  *(Train Journeys To Work)* variables have great influence on satisfaction with public transport. 

# References

- Jim Hester and Hadley Wickham, (2020). *fs: Cross-Platform File System Operations Based on 'libuv'.* R package version 1.5.0

- Kuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.

- Wickham et al., (2019). *Welcome to the tidyverse. Journal of Open Source Software*, e, 4(43), 1686 
