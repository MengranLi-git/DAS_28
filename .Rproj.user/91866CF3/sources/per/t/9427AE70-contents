---
title: "**Factors influencing Scots' satisfaction with public transport**"
author: "*Group 28: Aishwin Tikku, Mengran Li, Steven Kwok, Shaoquan Li, Shuning Li*"
output:

  pdf_document:
    latex_engine: pdflatex
    number_sections: no
    keep_tex: true
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
```

```{r libraries}
library(tidyverse)
library(tidymodels)
library(GGally)
library(car)
library(gvlma)
library(fs)
library(gvlma)
library(skimr)
library(kableExtra)
```

```{r echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
Data <- read.csv("data/Group_28.csv")
Data <- Data %>% spread(variable, value)
Data$DateCode <- as.factor(Data$DateCode)
```

# Introduction {#sec:Intro}
Public transport is necessary for most Scottish people. Therefore, its comfort and customer satisfaction are important for operators. To help improve public transport, The purpose is to discover factors having correlation with passengers' satisfaction in this project.
The data are from Scotlandâ€™s official statistics. The theme of Transport contains seven data sets, Road Transport Expenditure, Public Transport, Road Vehicles, Concessionary Travel Cards, Road Network and Traffic, Travel to Work and Other Purposes. There are 460 observations of 24 variables in this research.Find the best results through "Model diagnosis" , "stepwise regression" and comparing the selected model with full model on $adj$ $R^2$, AIC and BIC.

# Exploratory Data Analysis {#sec:EDA}
The whole data set, combined from seven data sets obtained from website of Scottish government statistics, has 24 factors and 460 observations. 
The explanatory variables are Road Casualties, Road Transport Expenditure, Public Transport, Road Vehicles, Concessionary Travel Cards, Road Network and Traffic and Travel to work and other purposes.

Firstly, filter and merge data.

```{r, echo = TRUE, eval = FALSE, warning = FALSE, message = FALSE}
#### data wrangling ####
data_dir <- "data"
# read data document name
csv_files <- fs::dir_ls(data_dir, regexp = "\\.csv$")
# dataset name
dataset_name <- c(
  "Casualties", "Expenditure", "Transport",
  "Vehicles", "Cards", "Network", "Purposes"
)
# read data and assign names to dataset
for (i in 1:7) {
  assign(dataset_name[i], readr::read_csv(csv_files[i]))
}
# select total number of casualty outcomes
Casualties <- Casualties %>%
  select(-c(Measurement, Units)) %>%
  filter(Outcome == "Killed Or Seriously Injured") %>%
  spread(Outcome, Value) %>%
  filter(Age == "All" & Gender == "All") %>%
  select(-c(Age, Gender))

# rename variable name
Expenditure <- Expenditure %>%
  select(-c(Measurement, Units)) %>%
  rename(Expenditure = Value)

# remove variables at the whole Scotland level
Transport <- Transport %>%
  select(-c(Measurement, Units)) %>%
  filter(`Indicator (public transport)` %in% c(
    "Number Of Passenger Train Stations",
    "Percentage Of Adults Reporting that they are Very or Fairly Satisfied with Public Transport"
  )) %>%
  spread(`Indicator (public transport)`, Value)

Vehicles <- Vehicles %>%
  select(-c(Measurement, Units)) %>%
  spread(`Indicator (road vehicles)`, Value)

# retain card numbers of all people
Cards <- Cards %>%
  select(-c(Measurement, Units)) %>%
  spread(Age, Value) %>%
  select(-`60 years and over`) %>%
  rename(Cards = All)

Network <- Network %>%
  select(-c(Measurement, Units)) %>%
  spread(`Indicator (road network traffic)`, Value)

Purposes <- Purposes %>%
  select(-c(Measurement, Units)) %>%
  spread(`Indicator (travel to work)`, Value)

# merge all dataset into one complete dataset by 'FeatureCode' and 'DateCode'
Data <- Expenditure %>%
  left_join(Casualties,
    by = c(
      "FeatureCode" = "FeatureCode",
      "DateCode" = "DateCode"
    )
  ) %>%
  left_join(Cards,
    by = c(
      "FeatureCode" = "FeatureCode",
      "DateCode" = "DateCode"
    )
  ) %>%
  left_join(Network,
    by = c(
      "FeatureCode" = "FeatureCode",
      "DateCode" = "DateCode"
    )
  ) %>%
  left_join(Purposes,
    by = c(
      "FeatureCode" = "FeatureCode",
      "DateCode" = "DateCode"
    )
  ) %>%
  left_join(Transport,
    by = c(
      "FeatureCode" = "FeatureCode",
      "DateCode" = "DateCode"
    )
  ) %>%
  left_join(Vehicles,
    by = c(
      "FeatureCode" = "FeatureCode",
      "DateCode" = "DateCode"
    )
  )

# rename variables
names(Data) <- c(
  "FeatureCode", "DateCode", "Expenditure", "Killed_Injured",
  "Cards", "Congestion", "Repair", "Mileage", "Work_Bus",
  "Business", "School", "Commuting", "Work_Cycling", "Education",
  "Health", "Shopping", "Work_Train", "Work_Walking", "Train_Stations",
  "Satisfaction", "One_Car", "More_Car", "Without_Car", "Petrol_Diesel"
)

# Transform the DateCode as a factor
Data$DateCode <- as.factor(Data$DateCode)
```

We get a complete data set called Data, and summarize histogram plots (Fig. \ref{fig:his}) are illustrated to detect data patterns. 

```{r,fig.align='center', echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, fig.cap="\\label{fig:his} Histogram plot of variables"}
# density plot
Data[, -c(1, 2)] %>%
  gather(key = "variable", value = "value") %>%
  ggplot() +
  geom_histogram(aes(x=value), fill="#80C687", color="#80C687", alpha=0.8) +
  facet_wrap(~variable, scales = "free") + 
  theme(axis.text.x= element_text(size = 4),
        axis.text.y= element_text(size = 4))
```

```{r, fig.align='center',echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, fig.cap="\\label{fig:scatter} Scatter plot of dependent variabls vs response variable"}
# scatter plot
Data[, -c(1, 2)] %>%
  gather(key = "variable", value = "value") %>%
  mutate(Satisfaction = (Data[, -c(1, 2)] %>% pull(Satisfaction) %>% rep(22))) %>%
  ggplot() +
  geom_point(aes(x = value, y = Satisfaction), fill="#80C687", color="#80C687", alpha=0.8) +
  facet_wrap(~variable, scales = "free") +
  theme(axis.text.x= element_text(size = 4),
        axis.text.y= element_text(size = 4))
```

The scatter (Fig. \ref{fig:scatter}) and correlation (Fig. \ref{fig:cor}) plots are proposed to explore the relationship among variables. We notice that these are some extreme values in the variables of "Cards", "Expenditure", "Killed_Injured", "Mileage", "Petrol_Diesel", "Train_Stations", which should be kept in mind. We also find the "More_Car", "Without_Car", "Work_Bus", "Work_Train" have a linear relationship with Satisfaction.

```{r, fig.width=9.5, fig.align='center',echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, fig.cap="\\label{fig:cor} Correlation plot of variabls"}
# correlation plot
corr <- cor(na.omit(Data[, -c(1, 2)]))

x <- rep(names(Data[, -c(1, 2)]))
y <- rep(names(Data[, -c(1, 2)]))
corrplot <- expand.grid(X = x, Y = y)
corrplot$Z <- c(corr)

ggplot(corrplot, aes(X, Y, fill = Z)) +
  geom_tile(aes(fill = Z, alpha = 0.7), show.legend = FALSE) +
  geom_text(aes(label = round(Z, 2))) +
  scale_fill_gradient2(
    low = muted("#80C687"),
    mid = "white",
    high = muted("#FF4500"),
    midpoint = 0,
    breaks = c(-1, 0, 1), labels = c(-1, 0, 1),
    limits = c(-1, 1)
  ) +
  theme(
    axis.text.x = element_text(
      angle = 30,
      vjust = .8,
      hjust = 0.8
    ),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )
```

Through the correlation plot (Fig. \ref{fig:cor}), there is a strong linear relationship between "Petrol_Diesel" "Train_Stations", "Mileage", "Expenditure", "Killed_Injured" and "Cards". 
Then we analyse the regression diagnosis results (Fig. \ref{fig:dia}) of model1 with all variables.

```{r, fig.align='center',echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
#### linear regression ####
# full model
fit <- lm(Satisfaction ~ ., data = na.omit(Data[, -1]))
```

Plot the Fitted values against Residuals and Q-Q plot to assess our assumptions.

```{r, fig.align='center',echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, fig.cap="\\label{fig:dia} Regression diagnosis results"}
# Diagnostic chart
par(mfrow = c(2, 2))
plot(fit)
```

-   Residuals vs Fitted plot (left top) shows that there is no systematic correlation between the residual value and the fitting value.

-   Normal Q-Q plot (right top) shows the points on the graph fall on a straight line with an angle of 45 degrees, which indicates the assumption of normality is not violated.

-   Scale-location plot (left bottom) displays the points around the horizontal line are randomly distributed. The invariant variance assumption is satisfied.

-   Residuals vs leverage (right bottom) figures out special observations.

-   According to the correlation plot and VIF, there is obvious multicollinearity among variables.

# Formal Data Analysis {#sec:FDA}
Linear regression model, a model for examining and discovering relations between the response variable and explanatory variable(s), is applied in this project. 
The Satisfaction is the response variable, the DateCode is the control variable and others are independent variables. A linear regression model is applied as:
$$y=\beta_0+\beta_1x_1+\dots+\beta_px_p+\varepsilon $$
where y is the response variable, $x_1$ to $x_p$ is the number of columns selected from 1 to p, $\beta_0$ is the intercept of data, $\beta_1$ to $\beta_p$ is the coefficients of corresponding columns of X from 1 to p. The $\varepsilon$ is the error terms of the estimations.

```{r ,echo = TRUE, eval = FALSE, warning = FALSE, message = FALSE}
#library(MASS)
library(MASS)
stepAIC(fit, na.rm = TRUE)

# 3 4 5 8 24
# cards, killed_injured, expenditure, petrol_diesel, mileage are highly correlated.
# model 2 without
names(Data)[c(3, 4, 5, 8, 24)]

fit2 <- lm(Satisfaction ~ DateCode + Cards + Repair + Work_Bus + 
             School + Health + Work_Train + Train_Stations + Without_Car + 
             Petrol_Diesel, data = na.omit(Data[, -1]))
summary(fit2)

#### Model selection ####

glance(fit)
glance(fit2)
```

A total of four regressions were performed. First, all the independent variables were regressed, and then the highly correlated factors were sequentially removed, and finally, four models were obtained. The variables with high correlation are removed by stepwise regression. Step AIC method is applied as a criterion for model selection. model 2 is the best choice.
Stepwise regression technique is applied to model selection. The selected model is as Table. \ref{tab:model2}. Control variable "DateCode" isn't displayed.

```{r}
fit2 <- lm(Satisfaction ~ Cards + Repair + Work_Bus + 
             School + Health + Work_Train + Train_Stations + Without_Car + 
             Petrol_Diesel, data = na.omit(Data[, -1]))
tidy(fit2) %>% 
  filter(!term %in% c("DateCode2012","DateCode2014","DateCode2015",
                     "DateCode2016","DateCode2017","DateCode2018")) %>%
  kable(caption = 'Model2, selected by Stepwise regression\\label{tab:model2}')
```

Model diagnosis are carried out to check model assumptions. Stepwise regression is applied to select variables with AIC as the criterion. Compare the selected model with full model on $adj$ $R^2$, AIC and BIC.
Model2 has higher $adj$ $R^2$ and smaller AIC and BIC compared with the model1 as Table. \ref{tab:com}.

```{r}
glance(fit) %>% 
  full_join(glance(fit2)) %>%
  dplyr::select(., c(adj.r.squared, AIC, BIC)) %>% 
  add_column(model = c("model 1", "model 2"), .before = "adj.r.squared") %>%
  kable(caption = 'Comparison of model1 with model2 on adj R2, AIC and BIC\\label{tab:com}')
```

Therefore the model2 is better.

To obtain robust results, using Bootstrap to select significant variables at 95% level and obtain its confidence interval of parameter estimation and their observations.This process should be repeated 1000 times.

```{r, out.width='80%', fig.cap="Density plots of parameters via bootstrap\\label{fig:boot}", fig.height = 5, fig.align='center'}
boot_models <- bootstraps(Data[, -1], times = 1000, apparent = TRUE) %>%
  mutate(
    model = map(splits, ~ lm(Satisfaction ~ DateCode + Cards + Repair + Work_Bus + 
                               School + Health + Work_Train + Train_Stations + Without_Car + 
                               Petrol_Diesel, data = .)),
    coef_info = map(model, tidy)
  )

boot_coefs <- boot_models %>%
  unnest(coef_info)

ci <- int_pctl(boot_models, coef_info)
# significant at 0.05
sig <- ci[sign(ci[, 2]) == sign(ci[, 4]), ] %>% pull(term)

boot_coefs <-  boot_coefs %>%
  mutate(sig = ifelse(term %in% sig, "TRUE", "FALSE")) %>%
  filter(term %in% c("Cards", "Health", "Petrol_Diesel","Repair", "School", "Train_Stations", "Without_Car", "Work_Bus", "Work_Train"))

boot_coefs2 <- boot_coefs %>%
  group_by(term) %>%
  summarise(est = median(estimate), lower = quantile(estimate, 0.025), upper = quantile(estimate, 0.975))

boot_coefs %>%
  ggplot() +
  geom_density(aes(estimate, fill = sig), alpha = 0.7,  color = "white") +
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = est))+
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = lower), color="Red", lty = 2)+
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = upper), color="Red", lty = 2)+
  geom_vline(data = boot_coefs2, mapping = aes(xintercept = 0), color="Blue", lty = 2)+
  facet_wrap(~term, scales = "free")+
  theme(axis.text.x= element_text(size = 6),
        axis.text.y= element_text(size = 6))
```
The density plots (Fig. \ref{fig:boot}) of parameters are displayed. The variables with orange are not significant while the variables with blue are significant at the $\alpha=0.05$. The blue dashed lines are zero and the orange dashed lines are 95% CI of parameters. The black lines are estimations of parameters.

# Conclusions {#sec:Conc}
- The variables Cards (Number of concessionary cards issued to all adults), Repair (The Percentage of Roads Needing Repairs), Work_Bus (Bus Journeys To Work) and Work_Train (Train Journeys To Work) have positive influence on satisfaction with public transport. The School (Child Journeys To School By Walking/Cycling) and Train_Stations (Number of Train Stations) have negative relationship with satisfaction with public transport.

- The reason that more roads needing repairs and less train stations come with higher satisfaction needs to be further explored.

# References {#sec:Ref}
- Jim Hester and Hadley Wickham, (2020). *fs: Cross-Platform File System Operations Based on 'libuv'.* R package version 1.5.0

- Kuhn et al., (2020). *Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles*.

- Wickham et al., (2019). *Welcome to the tidyverse. Journal of Open Source Software*, e, 4(43), 1686 
